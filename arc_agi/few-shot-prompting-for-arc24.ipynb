{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":200567623,"sourceType":"kernelVersion"},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900,"modelId":1902},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106},{"sourceId":78461,"sourceType":"modelInstanceVersion","modelInstanceId":65946,"modelId":90791},{"sourceId":81881,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Few-shot prompting for ARC24 Challenge","metadata":{}},{"cell_type":"markdown","source":"## Goal","metadata":{}},{"cell_type":"markdown","source":"How far can we get by simply using few-shot prompting with LLMs?","metadata":{}},{"cell_type":"markdown","source":"Questions to answer:\n\n- **What is the maximum context lenght that we can use at inference?** 61k tokens when using VLLM and phi-3\n- **What is the best way to encode the grids?** Use a minimal encoding to reduce token usage\n- **What is the effect of using a bigger number of examples?** Phi-3 does not benefit from using few-shot prompting\n- **Does giving a text description of the task helps?** Phi-3 does not benefit from including text description in the few-shot examples\n- **What is the best LLM for this task?** So far Llama 3 has been the only model capable of solving one test task.","metadata":{}},{"cell_type":"markdown","source":"## Results\n\n### Zero-shot baseline\n\nOn a first step I have tried a very simple baseline where I give input grids to the assistant and\nthe assistant replies with the output for each grid. This is done with all the train samples until\nwe give the test input and use the response of the model as the prediction.\nIn addition I also use data augmentations (flips and rotations) to make up to two predictions for each task.\nThe data augmentation is also useful because sometimes the prediction of the model is invalid, so we have to make multiple predictions to have 2 valid responses.\n\n| train | evaluation | test |\n|-------|------------|------|\n| 6.40% | 2.50%      | 0%   |\n\nThis approach is able to solve some of the train and evaluation task, but it does not solve any of the test tasks.\n\n### Few-shot results\n\nUsing samples from the evaluation dataset I have evaluated the effect of using few-shot prompting. In this case\nI have changed the prompt style: the user shows input-output pairs to the assistant and then requests the assistant\nto predict the output given some input.\n\n| n shots | accuracy | correct_pixels | correct_size | unanswered |\n|---------|----------|----------------|--------------|------------|\n| 0       | 5.80%    | 55.10%         | 73.50%       | 17.40%     |\n| 1       | 4.50%    | 44.80%         | 61.00%       | 23.60%     |\n| 2       | 4.80%    | 37.70%         | 54.40%       | 29.80%     |\n| 4       | 2.50%    | 22.40%         | 33.10%       | 33.10%     |\n| 8       | 2.30%    | 23.10%         | 35.50%       | 36.80%     |\n\nThe results show that Phi-3 does not benefit from few-shot prompting with ARC tasks. As we give more\nexamples the results get worse.\n\n### Add reasoning\n\nI have manually described with text the transformation of some of the evaluation tasks. Then repeat\nthe few-shot experiment but adding the reasoning before creating the grid.\n\n| uses reasoning | accuracy | correct_pixels | correct_size | unanswered |\n|----------------|----------|----------------|--------------|------------|\n| No             | 2.50%    | 22.40%         | 33.10%       | 33.10%     |\n| Yes            | 1%       | 19%            | 30.70%       | 42.50%     |\n\nThe model does not understand the puzzles. The examples and reasoning are not useful\n\n### Different models, zero-shot\n\nSince the best results were obtained for the 0-shot setup, I could try using different models.\nI can make submissions without using compute time, so I could see if some of the models is able\nto solve some task from the test set.\n\n| model      | test |\n|------------|------|\n| Phi-3      | 0    |\n| Mistral 7b | 0    |\n| Llama 3 8b | 1    |\n| Llama 3.1 8b | 0    |\n\nLlama 3 is able to solve one of the tasks from the test set. To better compare the models I should\nevaluate them on the public data, but I don't have Kaggle compute available.\n\n## Conclusion\n\nFew-shot or zero-shot inference with current LLMs is not the way to solve the ARC challenge.\nThe performance is very poor.","metadata":{}},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"### Notebooks","metadata":{}},{"cell_type":"markdown","source":"- https://www.kaggle.com/code/ironbar/2xvllm-with-code-interpreter\n- https://www.kaggle.com/code/ironbar/autobots-roll-out/notebook","metadata":{}},{"cell_type":"markdown","source":"### Models","metadata":{}},{"cell_type":"markdown","source":"- https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n- https://huggingface.co/blog/kv-cache-quantization\n- https://www.reddit.com/r/LocalLLaMA/comments/1e0kkgk/hardware_requirements_for_phi3_mini_and_phi3/\n- https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fmicrosoft-phi-3-3-8b-with-128k-context-released-on-hf-v0-h2xzg8vaigwc1.jpeg%3Fwidth%3D1734%26format%3Dpjpg%26auto%3Dwebp%26s%3Dec27de7bd97a90a4c44ff95c561ce8008ce7aed3\n- https://huggingface.co/docs/transformers/main/en/chat_templating","metadata":{}},{"cell_type":"markdown","source":"> The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets. ","metadata":{}},{"cell_type":"markdown","source":"## Learnings","metadata":{}},{"cell_type":"markdown","source":"### Phi-3 with transformers","metadata":{}},{"cell_type":"markdown","source":"If I load the phi-3 model with `torch_dtype=\"auto\"` and `device_map=\"balanced\"` it uses 3.7 GB of GPU memory on each device. It is able to handle an input of 4000 tokens using 13.4/15 GB of memory.\n\nThis is a very small context window compared to the 128k that the model is able to handle.","metadata":{}},{"cell_type":"markdown","source":"> ValueError: This model does not support the quantized cache. If you want your model to support quantized cache, please open an issue.","metadata":{}},{"cell_type":"markdown","source":"### Phi-3 with VLLM","metadata":{}},{"cell_type":"markdown","source":"It works with `vllm=0.5.3.post1` and `vllm-0.5.1`","metadata":{}},{"cell_type":"markdown","source":"I had to use `vllm-0.5.1` to make it work, other versions didn't worked so far. I was able to run VLLM with `Phi-3-mini-128k-instruct` and a context size of 61k by using `kv_cache_dtype='fp8_e5m2'`.\n\nThat is far from the 128k context size of the model, but much better than the 4k tokens when using transformers. Futhermore this [plot](https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fmicrosoft-phi-3-3-8b-with-128k-context-released-on-hf-v0-h2xzg8vaigwc1.jpeg%3Fwidth%3D1734%26format%3Dpjpg%26auto%3Dwebp%26s%3Dec27de7bd97a90a4c44ff95c561ce8008ce7aed3) says that a context size bigger than 44k does not work well.","metadata":{}},{"cell_type":"markdown","source":"#### What didn't work","metadata":{}},{"cell_type":"markdown","source":"I can makek it work with `vllm==0.4.3` and `Phi-3-mini-4k-instruct` with 4000 input tokens. However with `Phi-3-mini-128k-instruct` I get this error: \n\n```\nFile /opt/conda/lib/python3.10/site-packages/vllm/config.py:1224, in _get_and_verify_max_len(hf_config, max_model_len, disable_sliding_window, sliding_window_len)\n   1217 if disable_sliding_window:\n   1218     # TODO(robertgshaw): Find a model that supports rope_scaling\n   1219     # with sliding window to see if this case should be allowed.\n   1220     raise NotImplementedError(\n   1221         \"Disabling sliding window is not supported for models \"\n   1222         \"with rope_scaling. Please raise an issue so we can \"\n   1223         \"investigate.\")\n-> 1224 assert \"factor\" in rope_scaling\n   1225 scaling_factor = rope_scaling[\"factor\"]\n   1226 if rope_scaling[\"type\"] == \"yarn\":\n\nAssertionError: \n```","metadata":{}},{"cell_type":"markdown","source":"If I use `vllm-0.5.2` I get the same error for both models:\n\n```\nAttributeError: '_OpNamespace' '_C' object has no attribute 'rms_norm'\n```","metadata":{}},{"cell_type":"markdown","source":"If I use `vllm-0.5.1` it works for the 4k model. The 128k model gives OOM error.\n\nI try then using `tensor_parallel_size=2` and get this error:\n\n```\n# `tensor_parallel_size=2` \nValueError: The model's max seq len (64000) is larger than the maximum number of tokens that can be stored in KV cache (30528). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\n# `tensor_parallel_size=2` and kv_cache_dtype='fp8_e5m2',\nValueError: The model's max seq len (64000) is larger than the maximum number of tokens that can be stored in KV cache (61072). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\n```","metadata":{}},{"cell_type":"markdown","source":"#### Getting the same output as with transformers\n\nI have verified that if I apply the chat template before calling the model, and if I don't use kv cache quantization I get the [exact same response](https://www.kaggle.com/code/ironbar/phi-3-sample-code?scriptVersionId=188682799) to the default messages in the model documentation:\n\n```\n To solve the equation 2x + 3 = 7, follow these steps:\n\n1. Subtract 3 from both sides of the equation:\n   2x + 3 - 3 = 7 - 3\n   2x = 4\n\n2. Divide both sides of the equation by 2:\n   2x/2 = 4/2\n   x = 2\n\nSo, the solution to the equation 2x + 3 = 7 is x = 2.\n```","metadata":{}},{"cell_type":"markdown","source":"### First experiments","metadata":{}},{"cell_type":"markdown","source":"Using a basic grid encoder already uses 8k tokens in the worst case. (`30x30x2x4=7200`)\nThus an encoder that uses excel notation (A1, B2...) will use at least 3 times more tokens. ","metadata":{}},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class cfg:\n    # Model\n    #model_path = '/kaggle/input/phi-3/transformers/phi-3-mini-128k-instruct/1/Phi-3-mini-128k-instruct'\n    #model_path = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n    #model_path = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n    model_path = '/kaggle/input/llama-3.1/transformers/8b-instruct/1'\n    max_model_len = 8192 #61000 for phi-3\n    # Dataset\n    dataset_path = '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n    #dataset_path = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json'\n    #dataset_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n    n_tasks = None # Optional parameter to limit the number of task in the inference, set it to None to use all the tasks\n    # Few-shot\n    few_shot_dataset_path = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json'\n    n_shots = 0\n    # Inference params\n    max_predictions_per_task = 2 # \n    sampling_params = dict(temperature=0.0, max_tokens=2000) # https://docs.vllm.ai/en/latest/dev/sampling_params.html","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:44:38.443319Z","iopub.execute_input":"2024-07-25T12:44:38.443685Z","iopub.status.idle":"2024-07-25T12:44:38.455939Z","shell.execute_reply.started":"2024-07-25T12:44:38.443639Z","shell.execute_reply":"2024-07-25T12:44:38.455072Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nis_dry_run = cfg.dataset_path == '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json' and not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\nif is_dry_run:\n    print('This is a dry run, no inference nor installation of packages will be done')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:44:38.457608Z","iopub.execute_input":"2024-07-25T12:44:38.457942Z","iopub.status.idle":"2024-07-25T12:44:38.468223Z","shell.execute_reply.started":"2024-07-25T12:44:38.457915Z","shell.execute_reply":"2024-07-25T12:44:38.467449Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"%%time\nif not is_dry_run:\n    try:\n        import vllm\n    except ImportError:\n        !pip uninstall -q -y torch\n        !pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n    # model imports\n    from vllm import LLM, SamplingParams\n    from transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:44:38.469632Z","iopub.execute_input":"2024-07-25T12:44:38.469917Z","iopub.status.idle":"2024-07-25T12:47:37.578142Z","shell.execute_reply.started":"2024-07-25T12:44:38.469895Z","shell.execute_reply":"2024-07-25T12:47:37.577164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from abc import ABC, abstractmethod\nimport json\nimport os\nfrom tqdm.auto import tqdm\nimport numpy as np\nfrom itertools import islice, product\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nfrom termcolor import colored","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.579502Z","iopub.execute_input":"2024-07-25T12:47:37.580433Z","iopub.status.idle":"2024-07-25T12:47:37.593969Z","shell.execute_reply.started":"2024-07-25T12:47:37.580394Z","shell.execute_reply":"2024-07-25T12:47:37.593101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Code","metadata":{}},{"cell_type":"markdown","source":"## Grid encoding","metadata":{}},{"cell_type":"markdown","source":"There are many ways to encode/format the grid as input to the LLM. In this section we are going to define several encoders so we can sistematically try them all.","metadata":{}},{"cell_type":"code","source":"class GridEncoder(ABC):\n    @abstractmethod\n    def to_text(self, grid):\n        pass\n    \n    @abstractmethod\n    def to_grid(self, text):\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.597874Z","iopub.execute_input":"2024-07-25T12:47:37.598414Z","iopub.status.idle":"2024-07-25T12:47:37.604765Z","shell.execute_reply.started":"2024-07-25T12:47:37.598387Z","shell.execute_reply":"2024-07-25T12:47:37.604051Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_grid = np.eye(3, dtype=int).tolist()\n\ndef test_translator(translator):\n    assert sample_grid == translator.to_grid(translator.to_text(sample_grid))\n    print(translator.to_text(sample_grid))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.605939Z","iopub.execute_input":"2024-07-25T12:47:37.606274Z","iopub.status.idle":"2024-07-25T12:47:37.625022Z","shell.execute_reply.started":"2024-07-25T12:47:37.606243Z","shell.execute_reply":"2024-07-25T12:47:37.624359Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MinimalGridEncoder(GridEncoder):\n    @staticmethod\n    def to_text(grid):\n        text = '\\n'.join([''.join([str(x) for x in line]) for line in grid])\n        return text\n    \n    @staticmethod\n    def to_grid(text):\n        lines = text.strip().splitlines()\n        grid = [[int(x) for x in line] for line in lines]\n        return grid\n        \ntest_translator(MinimalGridEncoder())","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.626081Z","iopub.execute_input":"2024-07-25T12:47:37.626348Z","iopub.status.idle":"2024-07-25T12:47:37.638177Z","shell.execute_reply.started":"2024-07-25T12:47:37.626325Z","shell.execute_reply":"2024-07-25T12:47:37.637225Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GridWithSeparationEncoder(GridEncoder):\n    def __init__(self, split_symbol):\n        self.split_symbol = split_symbol\n\n    def to_text(self, grid):\n        text = '\\n'.join([self.split_symbol.join([str(x) for x in line]) for line in grid])\n        return text\n    \n    def to_grid(self, text):\n        lines = text.strip().splitlines()\n        grid = [[int(x) for x in line.split(self.split_symbol)] for line in lines]\n        return grid\n        \ntest_translator(GridWithSeparationEncoder('|'))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.639255Z","iopub.execute_input":"2024-07-25T12:47:37.640056Z","iopub.status.idle":"2024-07-25T12:47:37.650586Z","shell.execute_reply.started":"2024-07-25T12:47:37.640031Z","shell.execute_reply":"2024-07-25T12:47:37.649525Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GridCodeBlockEncoder(GridEncoder):\n    def __init__(self, base_encoder):\n        self.encoder = base_encoder\n    \n    def to_text(self, grid):\n        text = f'```grid\\n{self.encoder.to_text(grid)}\\n```'\n        return text\n    \n    def to_grid(self, text):\n        grid_text = text.split('```grid\\n')[1].split('\\n```')[0]\n        grid = self.encoder.to_grid(grid_text)\n        return grid\n        \ntest_translator(GridCodeBlockEncoder(MinimalGridEncoder()))\n\ntest_translator(GridCodeBlockEncoder(GridWithSeparationEncoder('|')))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.651853Z","iopub.execute_input":"2024-07-25T12:47:37.652177Z","iopub.status.idle":"2024-07-25T12:47:37.662924Z","shell.execute_reply.started":"2024-07-25T12:47:37.652148Z","shell.execute_reply":"2024-07-25T12:47:37.662042Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompting","metadata":{}},{"cell_type":"markdown","source":"There are also many ways to build a prompt for the ARC challenge. The class that builds the prompt will receive a grid encoder as input, this way we can try different prompts with different grid encoders. \nThe class that builds the prompts needs to be also capable of parsing the response from the model.","metadata":{}},{"cell_type":"code","source":"class PromptCreator(ABC):\n    def __init__(self, grid_encoder: GridEncoder):\n        self.grid_encoder = grid_encoder\n    \n    @abstractmethod\n    def create_task_prompts(self, task):\n        pass\n    \n    @abstractmethod\n    def parse_response(self, text):\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.666166Z","iopub.execute_input":"2024-07-25T12:47:37.666495Z","iopub.status.idle":"2024-07-25T12:47:37.675036Z","shell.execute_reply.started":"2024-07-25T12:47:37.666471Z","shell.execute_reply":"2024-07-25T12:47:37.674008Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimplePromptCreator(PromptCreator):\n\n    def create_task_prompts(self, task):\n        if cfg.model_path == '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1':\n            # Mistral does not have system prompt\n            messages = []\n        else:\n            messages = [ \n                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Your task is to answer to the user using always the same transformation of the user input.\"}, \n            ] \n        for sample in task['train']:\n            messages.append({\"role\": \"user\", \"content\": f\"Input:\\n{self.grid_encoder.to_text(sample['input'])}\"})\n            messages.append({\"role\": \"assistant\", \"content\": f\"Output:\\n{self.grid_encoder.to_text(sample['output'])}\"})\n\n        prompts = []\n        for test_sample in task['test']:\n            final_message = {\"role\": \"user\", \"content\": f\"Input:\\n{self.grid_encoder.to_text(test_sample['input'])}\"}\n            prompt = tokenizer.apply_chat_template(messages + [final_message],\n                                                   tokenize=False,\n                                                   add_generation_prompt=True)\n            prompts.append(prompt)\n        return prompts\n    \n    def parse_response(self, text):\n        grid_text = text.split('Output:\\n')[1]\n        return self.grid_encoder.to_grid(grid_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.676351Z","iopub.execute_input":"2024-07-25T12:47:37.676691Z","iopub.status.idle":"2024-07-25T12:47:37.690391Z","shell.execute_reply.started":"2024-07-25T12:47:37.676641Z","shell.execute_reply":"2024-07-25T12:47:37.689606Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"puzzle_explanations = {\n    '00576224': \"\"\"The pattern of the input is repeated to generate the output.\n\n1. The first two rows are obtained by simply repeating the 2x2 pattern 3 times along the cols axis.\n2. The following two rows are obtained by flipping the pattern horizontally and repeating it 3 times\n3. The final two rows are identical to the first ones, simply repeat the 2x2 pattern 3 times.\n\nThus the output is 3 times bigger than the input (6x6 vs 2x2) because the pattern is repeated 3 times in the row and col axis.\"\"\",\n    '009d5c81': \"\"\"To create the output we have to copy the input with two modifications:\n\n1. The object with color 1 is removed and replaced with the background color 0\n2. The color of the other object (there are only two objects in the grid) is modified.\n  The new color of this object depends on the shape of the object of color 1. There is a mapping\n  between shapes and colors. Just look at the train examples for an object of the same shape\n  and see the color that is applied on the output.\"\"\",\n    '00dbd492': \"\"\"The input shows a square with color 2 that is empty except from a point in the center.\nThe output is created by colorizing the inside of the square. The color is chosen depending on the size of the squares.\nThe larger square is painted with 3, the medium with 4 and the small with 8.\"\"\",\n    '03560426': \"\"\"The input shows objects of different colors at the bottom of the grid.\nThe output is created by moving the objects to the top left corner. The objects are moved from left to right order.\nThe first object is placed at the top left corner, the second object is placed at the lower right corner of the first object,\nthe third object is placed at the lower right corner of the second object and so on. There is oclusion between the objects,\nin those oclusions we see the rightmost object.\"\"\",\n    '0607ce86': \"\"\"This is a denoising task. The input shows the same object repeated many times, but there are noisy pixels in the grid.\nThe output is created by removing all the noise in the grid. The background should be completely 0.\nThe real object without noise can be guessed because there are many repetitions of the object, so we simply have to\nlook at the majority pixel on each location.\"\"\",\n    '0692e18c': \"\"\"The ouptut is created following this steps.\n\n1. The input is upscaled x3. So if the input is 3x3 the output should be an upscaled version of the input 9x9\n2. We apply an AND function in a sliding window fashion over the output using the inverted input pattern (take the input and swicth the background color 0 with the other color and viceversa)\n    \"\"\",\n    '070dd51e': \"\"\"The output is created by simply drawing horizontal and vertical lines between cells with the same color.\nIf there is an intersection between lines the vertical line will be shown.\"\"\",\n    '08573cc6': \"\"\"The output is created by drawing an spiral that starts at the cell with color 1.\nThe colors of the spiral are taken from the first two cells of the grid, which will be removed in the output.\"\"\",\n    '0a2355a6': \"\"\"The output is created by copying the input and changing the color of the objects.\nThe new color will be chosen depending on the number of holes of the object. There is a mapping between number of holes and color that can be observed from the input examples.\"\"\",\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.691737Z","iopub.execute_input":"2024-07-25T12:47:37.692047Z","iopub.status.idle":"2024-07-25T12:47:37.703539Z","shell.execute_reply.started":"2024-07-25T12:47:37.692018Z","shell.execute_reply":"2024-07-25T12:47:37.702264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FewShotPromptCreator(PromptCreator):\n    task_description = \"\"\"You are a helpful AI assistant. Your job is to solve tasks from the Abstraction and Reasoning Challenge (ARC). \nThe user will present you with sample input and output grids for each task. \nYour job will be to understand the transformation between the input and the output and apply it to the last input grid given by the user. \nThe puzzle-like inputs and outputs present a grid where each square can be one of ten colors. A grid can be any height or width between 1x1 and 30x30.\nThe background of the grid is typically colored with 0.\nThe tasks from ARC are based on the following priors:\n\n- Objectness: Objects persist and cannot appear or disappear without reason. Objects can interact or not depending on the circumstances.\n- Goal-directed: Objects can be animate or inanimate. Some objects are \"agents\" - they have intentions and they pursue goals.\n- Numbers & counting: Objects can be counted or sorted by their shape, appearance, or movement using basic mathematics like addition, subtraction, and comparison.\n- Basic geometry & topology: Objects can be shapes like rectangles, triangles, and circles which can be mirrored, rotated, translated, deformed, combined, repeated, etc. Differences in distances can be detected.\n\nThe transformations between input and output should be based on these priors.\n\"\"\"\n    def __init__(self, grid_encoder):\n        super().__init__(grid_encoder)\n        with open(cfg.few_shot_dataset_path, 'r') as f:\n            self.few_shot_tasks = json.load(f)\n        with open(cfg.few_shot_dataset_path.replace('challenges.json', 'solutions.json'), 'r') as f:\n            self.few_shot_solutions = json.load(f)\n        self.few_shot_tasks = {task_id: self.few_shot_tasks[task_id] for task_id in puzzle_explanations}\n        self.few_shot_solutions = {task_id: self.few_shot_solutions[task_id] for task_id in puzzle_explanations}\n        self.few_shot_task_ids = list(self.few_shot_tasks.keys())\n        self.n_shots = cfg.n_shots\n    \n    def create_task_prompts(self, task):\n        messages = [{\"role\": \"system\", \"content\": self.task_description}]\n        \n        for task_id in np.random.choice(self.few_shot_task_ids, self.n_shots):\n            few_shot_task = self.few_shot_tasks[task_id]\n            user_message = self.create_user_message_for_train_examples(few_shot_task)\n            for test_idx, test_sample in enumerate(few_shot_task['test']):\n                user_message += self.create_input_message('Test case', test_sample)\n                messages.append({\"role\": \"user\", \"content\": user_message})\n                user_message = ''\n                assistant_message = f'{puzzle_explanations[task_id]}\\n\\n' + self.create_output_message(self.few_shot_solutions[task_id][test_idx])\n                messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n\n        user_message = self.create_user_message_for_train_examples(task)        \n        prompts = []\n        for test_sample in task['test']:\n            user_message += self.create_input_message('Test case', test_sample)\n            messages.append({\"role\": \"user\", \"content\": user_message})\n            prompt = tokenizer.apply_chat_template(messages,\n                                                   tokenize=False,\n                                                   add_generation_prompt=True)\n            prompts.append(prompt)\n        return prompts\n    \n    def create_user_message_for_train_examples(self, task):\n        user_message = \"Let's see if you can solve this simple ARC task. These are some input-output grid examples that define the task.\\n\"\n        for example_idx, sample in enumerate(task['train']):\n            user_message += self.create_input_message(f'Example {example_idx}', sample)\n            user_message += '\\n' + self.create_output_message(sample['output'])\n        return user_message\n\n    def create_input_message(self, title, sample):\n        return f\"\\n## {title}\\n\\n### Input\\n\\n{self.grid_encoder.to_text(sample['input'])}\\n\"\n    \n    def create_output_message(self, grid):\n        return f\"### Output\\n\\n{self.grid_encoder.to_text(grid)}\\n\"\n    \n    def parse_response(self, text):\n        return self.grid_encoder.to_grid(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.705383Z","iopub.execute_input":"2024-07-25T12:47:37.705714Z","iopub.status.idle":"2024-07-25T12:47:37.725654Z","shell.execute_reply.started":"2024-07-25T12:47:37.70568Z","shell.execute_reply":"2024-07-25T12:47:37.72453Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_sample_prompt(data, prompt_creator):\n    prompts = [prompt_creator.create_task_prompts(task)[0] for task in data.values()]\n    prompts = sorted(prompts, key=lambda x: len(x))\n    pretty_print_prompt(prompts[0])\n    \ndef pretty_print_prompt(text):\n    color = 'black'\n    attrs = None\n    for line in text.splitlines():\n        if line.startswith('<|assistant|>'):\n            color = 'blue'\n        elif line.startswith('<|user|>'):\n            color = 'black'\n        elif line.startswith('<|system|>'):\n            color = 'green'\n            \n        if line.startswith('<'):\n            attrs = ['bold']\n        else:\n            attrs = None\n        print(colored(line, color, attrs=attrs))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.727577Z","iopub.execute_input":"2024-07-25T12:47:37.728896Z","iopub.status.idle":"2024-07-25T12:47:37.746911Z","shell.execute_reply.started":"2024-07-25T12:47:37.728868Z","shell.execute_reply":"2024-07-25T12:47:37.745859Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_input_token_length_distribution(data, prompt_creator):\n    prompts = []\n    for task in data.values():\n        prompts.extend(prompt_creator.create_task_prompts(task))\n    token_length_distribution = [len(tokenizer.tokenize(prompt)) for prompt in tqdm(prompts)]\n    plt.title('Prompt token length distribution')\n    plt.hist(token_length_distribution)\n    plt.xlabel('n tokens')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.749164Z","iopub.execute_input":"2024-07-25T12:47:37.749815Z","iopub.status.idle":"2024-07-25T12:47:37.763096Z","shell.execute_reply.started":"2024-07-25T12:47:37.749783Z","shell.execute_reply":"2024-07-25T12:47:37.761958Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"if not is_dry_run:\n    llm = LLM(model=cfg.model_path,\n              trust_remote_code=True, \n              dtype='half', \n              tensor_parallel_size=2, # to use 2 gpus\n              max_model_len=cfg.max_model_len,\n              kv_cache_dtype='fp8_e5m2',\n              enforce_eager=True, # without this 13.9GB of memory is used on each GPU, with this is 13.3GB,\n              disable_log_stats=True,\n             )\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)\n    for number in '0123456789':\n        print(f'{number}: {[key for key in tokenizer.get_vocab().keys() if number in key and not key.startswith(\"<\")]}')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:47:37.764166Z","iopub.execute_input":"2024-07-25T12:47:37.76597Z","iopub.status.idle":"2024-07-25T12:48:26.880834Z","shell.execute_reply.started":"2024-07-25T12:47:37.765935Z","shell.execute_reply":"2024-07-25T12:48:26.879628Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The tokenizer from phi-3 encodes each digit indepently, it does not group numbers such as 10 or 100.","metadata":{}},{"cell_type":"markdown","source":"### Data augmentation","metadata":{}},{"cell_type":"markdown","source":"We need data augmentation to make multiple predictions for each task.","metadata":{}},{"cell_type":"code","source":"class DataAugmentation():\n    def __init__(self, flip, n_rot90):\n        self.flip = flip\n        self.n_rot90 = n_rot90\n        \n    def augment_task(self, task):\n        augmented_task = dict()\n        for partition, samples in task.items():\n            augmented_task[partition] = [{name:self.augment_grid(grid) for name,grid in sample.items()} for sample in samples]\n        return augmented_task\n    \n    def augment_grid(self, grid):\n        grid = np.array(grid)\n        if self.flip:\n            grid = np.flip(grid, axis=1)\n        grid = np.rot90(grid, k=self.n_rot90)\n        return grid.tolist()\n    \n    def revert_augmentation(self, grid):\n        grid = np.array(grid)\n        grid = np.rot90(grid, k=-self.n_rot90)\n        if self.flip:\n            grid = np.flip(grid, axis=1)\n        return grid.tolist()\n\n\nfor flip in [True, False]:\n    for n_rot90 in range(4):\n        data_augmentation = DataAugmentation(flip, n_rot90)\n        assert sample_grid == data_augmentation.revert_augmentation(data_augmentation.augment_grid(sample_grid))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:26.882363Z","iopub.execute_input":"2024-07-25T12:48:26.882698Z","iopub.status.idle":"2024-07-25T12:48:26.894925Z","shell.execute_reply.started":"2024-07-25T12:48:26.882651Z","shell.execute_reply":"2024-07-25T12:48:26.89403Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plots","metadata":{}},{"cell_type":"code","source":"def plot_task(task):\n    samples = task['train'] + task['test']\n    for plot_idx, sample in enumerate(samples):\n        plt.subplot(2, len(samples), plot_idx + 1)\n        plot_grid(sample['input'])\n        if 'output' in sample:\n            plt.subplot(2, len(samples), plot_idx + 1 + len(samples))\n            plot_grid(sample['output'])\n            \ndef plot_grids(grids):\n    for plot_idx, grid in enumerate(grids):\n        plt.subplot(1, len(grids), plot_idx + 1)\n        plot_grid(grid)\n            \ndef plot_grid(grid):\n    grid = np.array(grid)\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    plt.imshow(grid, cmap=cmap, norm=norm)\n    plt.grid(True,which='both',color='lightgrey', linewidth=0.5) \n    plt.xticks(np.arange(-0.5, grid.shape[1]), [])\n    plt.yticks(np.arange(-0.5, grid.shape[0]), [])\n    plt.xlim(-0.5, grid.shape[1]-0.5)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:26.896212Z","iopub.execute_input":"2024-07-25T12:48:26.896539Z","iopub.status.idle":"2024-07-25T12:48:26.91353Z","shell.execute_reply.started":"2024-07-25T12:48:26.896509Z","shell.execute_reply":"2024-07-25T12:48:26.91265Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"def analyze_number_of_predictions_per_task(data, texts):\n    number_of_predictions = dict()\n    for task_id, task in data.items():\n        number_of_predictions[task_id] = len(texts[task_id]['responses'])/len(task['test'])\n    plt.title('Distribution of the number of predictions per task')\n    plt.hist(number_of_predictions.values(), bins=np.arange(1.5, 9))\n    plt.xlabel('number of predictions')\n    plt.ylabel('count')\n    return number_of_predictions","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:26.914675Z","iopub.execute_input":"2024-07-25T12:48:26.915004Z","iopub.status.idle":"2024-07-25T12:48:26.930061Z","shell.execute_reply.started":"2024-07-25T12:48:26.914971Z","shell.execute_reply":"2024-07-25T12:48:26.929181Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(ground_truth, solutions):\n    \"\"\"\n    Computes the following metrics:\n    \n    - Accuracy\n    - Correct pixels\n    - Correct size\n    \"\"\"\n    metrics = []\n    accuracy, correct_pixels, correct_size = [], [], []\n    for task_id, task_ground_truth in ground_truth.items():\n        task_metrics = []\n        plot_task(data[task_id]); plt.suptitle(f'{task_id}'); plt.show()\n        for idx, correct_grid in enumerate(task_ground_truth):\n            predicted_grids = list(solutions[task_id][idx].values())\n            predicted_grids = [grid for grid in predicted_grids if grid]\n            \n            task_metrics.append(evaluate_grid(correct_grid, predicted_grids))\n            print_metrics(task_metrics[-1], f'{task_id}_{idx}')\n            plot_grids([correct_grid] + predicted_grids)\n            plt.suptitle(f'{task_id}_{idx}')\n            plt.show()\n        metrics.append(average_metrics(task_metrics))\n    print('\\n'*3 + '# Aggregated metrics:')\n    print_metrics(average_metrics(metrics))\n    save_metrics(metrics, solutions)\n    plot_metrics_distribution(metrics)\n    print_metrics(average_metrics(metrics))\n    \ndef plot_metrics_distribution(metrics):\n    for key in metrics[0]:\n        values = [x[key] for x in metrics]\n        plt.title(f'Distribution of {key}')\n        plt.hist(values, bins=np.linspace(0, 1, 10))\n        plt.xlabel(key)\n        plt.ylabel('count')\n        plt.show()\n    \ndef average_metrics(metrics):\n    averaged_metrics = dict()\n    for key in metrics[0]:\n        averaged_metrics[key] = np.mean([x[key] for x in metrics])\n    return averaged_metrics\n        \ndef save_metrics(metrics, solutions):\n    formatted_metrics = dict(global_metrics=average_metrics(metrics))\n    for task_id, task_metrics in zip(solutions, metrics):\n        formatted_metrics[task_id] = task_metrics\n    with open('metrics.json', 'w') as f:\n        json.dump(formatted_metrics, f)\n\ndef print_metrics(metrics, prefix=''):\n    text = f'{prefix}'\n    for key, value in metrics.items():\n        text += f'{key}: {value*100:.1f}%\\t'\n    print(text)\n\n    \ndef evaluate_grid(correct_grid, predicted_grids):\n    correct_grid = np.array(correct_grid)\n    metrics = dict(accuracy=0, correct_pixels=0, correct_size=0, unanswered=(2 - len(predicted_grids))/2)\n    for predicted_grid in predicted_grids:\n        predicted_grid = np.array(predicted_grid)\n        if correct_grid.shape == predicted_grid.shape:\n            metrics['accuracy'] = max(metrics['accuracy'], np.all(predicted_grid == correct_grid))\n            metrics['correct_pixels'] = max(metrics['correct_pixels'], np.mean(predicted_grid == correct_grid))\n            metrics['correct_size'] = max(metrics['correct_size'], correct_grid.shape == predicted_grid.shape)\n    return metrics","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:26.931309Z","iopub.execute_input":"2024-07-25T12:48:26.931637Z","iopub.status.idle":"2024-07-25T12:48:26.949999Z","shell.execute_reply.started":"2024-07-25T12:48:26.931604Z","shell.execute_reply":"2024-07-25T12:48:26.949127Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"markdown","source":"We need to generate 2 different predictions for each task. The model could fail to generate a prediction, or the parsing can fail... Thus we need a method that is robust to fails.\n\nOne way to solve this would be to use data augmentation. By applying rotations and flips we could generate up to 8 variations of each task. So we could try with different data augmentations until we have 2 predictions for each task. Another alternative would be to make inference with the 8 variations and use majority voting.","metadata":{}},{"cell_type":"code","source":"def solve_task(task_id, task, prompt_creator, sampling_params):\n    data_augmentation_params = product([False, True], [0, 1, 2, 3])\n    solution = {task_id:[{\"attempt_1\": [], \"attempt_2\": []} for _ in task['test']]}\n    texts = dict(prompts=[], responses=[], exceptions=[])\n    for flip, n_rot90 in islice(data_augmentation_params, cfg.max_predictions_per_task):\n        data_augmentation = DataAugmentation(flip, n_rot90)\n        augmented_task = data_augmentation.augment_task(task)\n        prompts = prompt_creator.create_task_prompts(augmented_task)\n        outputs = llm.generate(prompts, sampling_params)\n        responses = [output.outputs[0].text for output in outputs]\n        for idx, response in enumerate(responses):\n            try:\n                augmented_grid = prompt_creator.parse_response(response)\n                grid = data_augmentation.revert_augmentation(augmented_grid)\n                if not solution[task_id][idx][\"attempt_1\"]:\n                    solution[task_id][idx][\"attempt_1\"] = grid\n                elif solution[task_id][idx][\"attempt_1\"] != grid and not solution[task_id][idx][\"attempt_2\"]:\n                    solution[task_id][idx][\"attempt_2\"] = grid\n            except Exception as e:\n                print(f'Exception when parsing response from {task_id}: {e}')\n                texts['exceptions'].append(str(e))\n        texts['prompts'].append(prompts)\n        texts['responses'].append(responses)\n        if is_solution_done(solution):\n            break\n    return solution, {task_id:texts}\n\ndef is_solution_done(solution):\n    for task_id, predictions in solution.items():\n        for prediction in predictions:\n            for grid in prediction.values():\n                if not grid:\n                    return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:26.951392Z","iopub.execute_input":"2024-07-25T12:48:26.952211Z","iopub.status.idle":"2024-07-25T12:48:26.96852Z","shell.execute_reply.started":"2024-07-25T12:48:26.952176Z","shell.execute_reply":"2024-07-25T12:48:26.967608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(data, prompt_creator, sampling_params):\n    solutions, texts = dict(), dict()\n    for task_id, task in tqdm(data.items(), total=len(data)):\n        task_solution, task_texts = solve_task(task_id, task, prompt_creator, sampling_params)\n        solutions.update(task_solution)\n        texts.update(task_texts)\n    return solutions, texts","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:26.969738Z","iopub.execute_input":"2024-07-25T12:48:26.970067Z","iopub.status.idle":"2024-07-25T12:48:26.983397Z","shell.execute_reply.started":"2024-07-25T12:48:26.970037Z","shell.execute_reply":"2024-07-25T12:48:26.982545Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(cfg.dataset_path) as f:\n    data = json.load(f)\nif cfg.n_tasks is not None:\n    data = dict(islice(data.items(), cfg.n_tasks))\nprint(f'There are {len(data)} tasks to solve.')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:26.984388Z","iopub.execute_input":"2024-07-25T12:48:26.984706Z","iopub.status.idle":"2024-07-25T12:48:27.090592Z","shell.execute_reply.started":"2024-07-25T12:48:26.984657Z","shell.execute_reply":"2024-07-25T12:48:27.089715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not is_dry_run:\n    #prompt_creator = FewShotPromptCreator(GridCodeBlockEncoder(MinimalGridEncoder()))\n    #prompt_creator = SimplePromptCreator(GridCodeBlockEncoder(MinimalGridEncoder()))\n    prompt_creator = SimplePromptCreator(GridCodeBlockEncoder(GridWithSeparationEncoder('|')))\n    print_sample_prompt(data, prompt_creator)\n    plot_input_token_length_distribution(data, prompt_creator)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:27.091903Z","iopub.execute_input":"2024-07-25T12:48:27.092196Z","iopub.status.idle":"2024-07-25T12:48:27.790012Z","shell.execute_reply.started":"2024-07-25T12:48:27.092172Z","shell.execute_reply":"2024-07-25T12:48:27.788949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if is_dry_run:\n    with open('submission.json', 'w') as f:\n        json.dump(dict(dry_run=True), f)\nelse:\n    sampling_params = SamplingParams(**cfg.sampling_params)\n    solutions, texts = inference(data, prompt_creator, sampling_params)\n    with open('submission.json', 'w') as f:\n        json.dump(solutions, f)    ","metadata":{"execution":{"iopub.status.busy":"2024-07-25T12:48:27.796471Z","iopub.execute_input":"2024-07-25T12:48:27.797826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not is_dry_run:\n    number_of_predictions_per_task = analyze_number_of_predictions_per_task(data, texts)\n    number_of_predictions_per_task","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"ground_truth_path = cfg.dataset_path.replace('challenges.json', 'solutions.json')\nif os.path.exists(ground_truth_path):\n    with open(ground_truth_path, 'r') as f:\n        ground_truth = json.load(f)\n    ground_truth = {key: ground_truth[key] for key in solutions}\n    evaluate(ground_truth, solutions)\n    \n    with open('texts.json', 'w') as f:\n        json.dump(texts, f)\n    with open('number_of_predictions_per_task.json', 'w') as f:\n        json.dump(number_of_predictions_per_task, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TODO","metadata":{}},{"cell_type":"markdown","source":"- [x] Do I have to apply the chat template before calling the model? Yes\n- [x] Implement a simple formatter and parser to go from ARC format to text and viceversa\n- [x] Function to format a sample and to parse the output\n- [x] Verify that the tokenizer does encode each cell independently. Using the tokenizer vocabulary I have verified that each number appears only once, thus separation between numbers is not necessary.\n- [x] Make inference in a whole dataset\n- [x] Evaluate dataset, I would like to see statistics about failed predictions and accuracy\n- [x] Visualization of the predictions\n- [x] Add a configuration object\n- [ ] Add logging\n- [x] Show a promtp as example\n- [x] Show the full task as reference\n- [x] Sampling parameters to configuration\n- [x] Improve saving of the metrics\n- [x] First evaluations with a smaller subset\n- [x] Token length distribution\n- [x] Dry run for submission, check on previous competitions how it is done. https://www.kaggle.com/discussions/product-feedback/315792\n- [x] Measure the number of predictions per task\n- [x] Better metrics\n- [x] Few shot prompts","metadata":{}}]}